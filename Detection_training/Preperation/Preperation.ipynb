{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing to compute with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating c:\\dev\\DHBW\\Studienarbeit\\Detection_training\\Tensorflow\\workspace\\images\n",
      "Creating c:\\dev\\DHBW\\Studienarbeit\\Detection_training\\Tensorflow\\workspace\\images\\collected_images\n",
      "Creating c:\\dev\\DHBW\\Studienarbeit\\Detection_training\\Tensorflow\\workspace\\images\\collected_images_resized\n",
      "Creating c:\\dev\\DHBW\\Studienarbeit\\Detection_training\\Tensorflow\\workspace\\images\\trainset\n",
      "Creating c:\\dev\\DHBW\\Studienarbeit\\Detection_training\\Tensorflow\\workspace\\images\\testset\n",
      "Creating c:\\dev\\DHBW\\Studienarbeit\\Detection_training\\Tensorflow\\workspace\\images\\devset\n",
      "Creating c:\\dev\\DHBW\\Studienarbeit\\Detection_training\\Tensorflow\\workspace\n",
      "Creating c:\\dev\\DHBW\\Studienarbeit\\Detection_training\\Tensorflow\\scripts\n",
      "Creating c:\\dev\\DHBW\\Studienarbeit\\Detection_training\\Tensorflow\\models\n",
      "Creating c:\\dev\\DHBW\\Studienarbeit\\Detection_training\\Tensorflow\\workspace\\annotations\n",
      "Creating c:\\dev\\DHBW\\Studienarbeit\\Detection_training\\Tensorflow\\workspace\\models\n",
      "Creating c:\\dev\\DHBW\\Studienarbeit\\Detection_training\\Tensorflow\\workspace\\pre-trained-models\n",
      "Creating c:\\dev\\DHBW\\Studienarbeit\\Detection_training\\Tensorflow\\workspace\\models\\tomatoDetection320x320\n",
      "Creating c:\\dev\\DHBW\\Studienarbeit\\Detection_training\\Tensorflow\\workspace\\models\\tomatoDetection320x320\\export\n",
      "Creating c:\\dev\\DHBW\\Studienarbeit\\Detection_training\\Tensorflow\\protoc\n",
      "Creating c:\\dev\\DHBW\\Studienarbeit\\Detection_training\\Tensorflow\\labelimg\n"
     ]
    }
   ],
   "source": [
    "import Paths\n",
    "\n",
    "# setting up the paths\n",
    "working_paths = Paths.WorkingPaths()\n",
    "working_paths.setup_paths()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling own images with Label Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "!pip install --upgrade pyqt5 lxml\n",
    "\n",
    "if os.listdir(working_paths.LABELIMG_PATH).__len__ == 0:\n",
    "    !git clone https://github.com/tzutalin/labelImg {working_paths.LABELIMG_PATH}\n",
    "\n",
    "if os.name == 'posix':\n",
    "    !cd {working_paths.LABELIMG_PATH} && make qt5py3\n",
    "if os.name =='nt':\n",
    "    !cd {working_paths.LABELIMG_PATH} && pyrcc5 -o libs/resources.py resources.qrc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {working_paths.LABELIMG_PATH} && python labelImg.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading images from other sources\n",
    "\n",
    "First i downloaded the images and xml-annotations from this [github repository](https://github.com/datitran/raccoon_dataset).\n",
    "\n",
    "You can simply copy them in the `collected_images` directory which should be available under the `Tensorflow/workspace/images` directory. Now you should have about 200 images from Racoons pretty low number isn't it?\n",
    "\n",
    "Well thats true so we will increase them. But for a first evaluation of choosing the right pretrained model it should be fine to use. Especially if you want to use Google Colab since you must likely an only train for a couple of hours before you run out of GPU."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the images for better performance during trainings.\n",
    "\n",
    "downscaling thm now is better than when TensorFlow would do it on runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_data import resize_images, preprocess_annotations\n",
    "\n",
    "resize_images(working_paths.COLLECTED_IMAGES_PATH, working_paths.RESIZED_IMAGES_PATH, 320)\n",
    "preprocess_annotations(working_paths.COLLECTED_IMAGES_PATH, working_paths.RESIZED_IMAGES_PATH, 320)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split up the resized images into Dev- Test- and Trainset\n",
    "\n",
    "You can do it manually or using the following Code snippet to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# First delete existing files in the directory for the datasets\n",
    "def delete_datasets(directory: str):\n",
    "    for f in os.listdir(directory):\n",
    "        os.remove(os.path.join(directory, f))\n",
    "\n",
    "delete_datasets(working_paths.DEVSET_PATH)\n",
    "delete_datasets(working_paths.TESTSET_PATH)\n",
    "delete_datasets(working_paths.TRAINSET_PATH)\n",
    "\n",
    "all_files: dict[list] = {}\n",
    "# Getting all the files in the directory mapped to the basenames (img, xml-file)\n",
    "for file in os.listdir(working_paths.RESIZED_IMAGES_PATH):\n",
    "    filename = file.split('.', 1)[0]\n",
    "    if filename in all_files:\n",
    "        all_files[filename].append(file)\n",
    "    else:\n",
    "        all_files[filename] = [file]\n",
    "\n",
    "# determine the ratio  for dev- and testset files\n",
    "testset_files = 0.1\n",
    "devset_files = 0.1\n",
    "# rest will be used for training\n",
    "\n",
    "position = 0\n",
    "output_dir = working_paths.TESTSET_PATH\n",
    "# lets start with testset files\n",
    "for filename, names in all_files.items():\n",
    "    if position >= testset_files * len(all_files):\n",
    "        output_dir = working_paths.DEVSET_PATH\n",
    "    if position >= (testset_files + devset_files) * len(all_files):\n",
    "        output_dir = working_paths.TRAINSET_PATH\n",
    "\n",
    "    for name in names:\n",
    "        original_path = os.path.join(working_paths.RESIZED_IMAGES_PATH, name)\n",
    "        output_path = os.path.join(output_dir, name)\n",
    "        os.replace(original_path, output_path)\n",
    "    position += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compress Datasets to use on Google Colab and on other platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tar: Removing leading drive letter from member names\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "command = \"{} {} {} {}\".format(os.path.join(working_paths.IMAGE_PATH, 'dataset.tar.gz'), working_paths.TRAINSET_PATH, working_paths.TESTSET_PATH, working_paths.DEVSET_PATH)\n",
    "!tar -czf {command}\n",
    "# If you want to export the dataset you need to manually copy it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "racon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09d6544f8d79fabd800242fcd55352230e534831db91f1f5325329e9514e4075"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
