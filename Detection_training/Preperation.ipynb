{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing to compute with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Tensorflow.scripts.Paths as Paths\n",
    "import os\n",
    "\n",
    "# setting up the paths\n",
    "paths = Paths.WorkingPaths\n",
    "paths.setup_paths()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling own images with Label Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install label-studio --user"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!label-studio start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: when finished you will export them to Pascal VOC Annotation and manually copy them into the `collected_images` directory in the `Tensorflow/workspace/images` directory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading images from other sources\n",
    "\n",
    "First i downloaded the images and xml-annotations from this [github repository](https://github.com/datitran/raccoon_dataset).\n",
    "\n",
    "You can simply copy them in the `collected_images` directory which should be available under the `Tensorflow/workspace/images` directory. Now you should have about 200 images from Racoons pretty low number isn't it?\n",
    "\n",
    "Well thats true so we will increase them. But for a first evaluation of choosing the right pretrained model it should be fine to use. Especially if you want to use Google Colab since you most likely only train for a couple of hours before you run out of GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming the files to match the others\n",
    "import uuid\n",
    "\n",
    "renamed: dict[str, str] = {}\n",
    "for filenames in os.listdir(paths.COLLECTED_IMAGES_PATH):\n",
    "    filename, ext = os.path.splitext(filenames)\n",
    "    if 'raccoon' in filename:\n",
    "        if ext != '.xml':\n",
    "            # Generate 60 Bit uuid\n",
    "            uuid_ = uuid.uuid4().hex[:16]\n",
    "            renamed.update({filename: uuid_})\n",
    "        os.rename(os.path.join(paths.COLLECTED_IMAGES_PATH, filename + ext), os.path.join(paths.COLLECTED_IMAGES_PATH, renamed[filename] + ext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the xml files to csv\n",
    "# this will also convert your images from Label Studio\n",
    "!python {Paths.XML_TO_CSV} -i {paths.COLLECTED_IMAGES_PATH} -o {Paths.CSV_FILE}\n",
    "# The label from the github source are named raccoon. The script will automatically makes Raccoon out of it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download openimages from google\n",
    "\n",
    "We will use [fiftyone](https://docs.voxel51.com/user_guide/dataset_zoo/datasets.html#open-images-v7) to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone\n",
    "\n",
    "import fiftyone as fo\n",
    "\n",
    "# Customize where zoo datasets are downloaded\n",
    "fo.config.dataset_zoo_dir = paths.IMAGE_PATH\n",
    "\n",
    "# This will take a hell lot of time since the .csv file will be downloaded for all labels and images (train has 2.2 GB)\n",
    "dataset = fo.zoo.load_zoo_dataset(\n",
    "    \"open-images-v7\",\n",
    "    label_types=[\"detections\"],\n",
    "    classes=[\"Raccoon\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will need to preprocess that huge CSV files and combine them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = \"\"\n",
    "for label in Paths.LABELS:\n",
    "    labels +=\" -l \" + label[\"name\"]\n",
    "\n",
    "!cargo run --manifest-path={Paths.CSV_CONV} --release -- -i {Paths.OPEN_IMAGES_TRAIN} -o {paths.COLLECTED_IMAGES_PATH} {labels}\n",
    "!cargo run --manifest-path={Paths.CSV_CONV} --release -- -i {Paths.OPEN_IMAGES_TEST} -o {paths.COLLECTED_IMAGES_PATH} {labels}\n",
    "!cargo run --manifest-path={Paths.CSV_CONV} --release -- -i {Paths.OPEN_IMAGES_VALIDATION} -o {paths.COLLECTED_IMAGES_PATH} {labels}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the images for better performance during trainings.\n",
    "\n",
    "downscaling thm now is better than when TensorFlow would do it on runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tensorflow.scripts.preprocessing_data import resize_images\n",
    "# %pip install imutils\n",
    "\n",
    "!cargo run --manifest-path={Paths.CSV_RESIZE} --release -- -i {Paths.CSV_FILE} -o {Paths.CSV_FILE_RESIZED} -r 320\n",
    "\n",
    "resize_images(paths.COLLECTED_IMAGES_PATH, paths.RESIZED_IMAGES_PATH, 320)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split up the resized images into Dev- Test- and Trainset\n",
    "\n",
    "You can do it manually or using the following Code snippet to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some reasons bugged. I can not directly use the command\n",
    "!cargo run --manifest-path={Paths.SPLIT_DATASET} --release -- -i {paths.RESIZED_IMAGES_PATH} -o {paths.IMAGE_PATH}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compress Datasets to use on Google Colab and on other platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths will be relative in tar files. The tar command would take absolute paths and by using the .tar file paths would be wrong.\n",
    "trainset = os.path.join('Tensorflow', 'workspace', 'images', 'trainset')\n",
    "dataset = os.path.join('Tensorflow', 'workspace', 'images', Paths.DATASET_NAME)\n",
    "testset = os.path.join('Tensorflow', 'workspace', 'images', 'testset')\n",
    "devset = os.path.join('Tensorflow', 'workspace', 'images', 'devset')\n",
    "\n",
    "command = \"{} {} {} {}\".format(dataset, trainset, testset, devset)\n",
    "!tar -czf {command}\n",
    "# If you want to export the dataset you need to manually copy it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23fe2c85cd4f0d6d4635f96a95cebf9dfe9e68035876b476cfa94a759db7b2c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
